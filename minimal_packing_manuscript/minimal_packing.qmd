---
title: Minimal discrete matches for target grain size distributions
execute:
  echo: false
  warning: false
format:
  html:
    code-fold: true
  docx:
    toc: false
    number-sections: false
    reference-doc: manuscript_template.docx
  pdf:
    pdf-engine: pdflatex
author:
  - name: Lorne Arnold
    orcid: 0000-0002-6993-1642
    email: arnoldl@uw.edu
    affiliation: University of Washington Tacoma
    corresponding: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: sentence
editor_options: 
  chunk_output_type: console
---

## Abstract

Soil is fundamentally a discrete material that is, nevertheless, commonly modeled as a continuum in part because of the computational expense of large-scale discrete element models (DEMs).
Even at lab specimen scales, DEMâ€™s computational cost may be substantial depending on the grain sizes being modeled.
Despite these limitations, discrete models have proven useful in furthering our understanding of soil mechanics because they can spontaneously replicate realistic soil behavior as an emergent macro-scale property from a collection of particles following relatively simple interaction rules.
This makes DEM an attractive tool for a multi-scale modeling approach where the constitutive behavior of a representative volume element (RV) is characterized with a discrete model and applied at a larger scale through a continuum model.
The ability of the RV to represent a soil depends strongly on an appropriate grain size distribution match.
However, in order to achieve computationally feasible models, even at lab scales, DEM simulations often use larger minimum particle sizes and more uniform distributions than their intended targets.
Intuitively, discrete matches of different grain size distributions (GSDs) will require vastly different numbers of particles.
But the precise relationship between GSD characteristics and the number of particles needed to match the distribution (and by extension the associated computational cost associated) is not intuitive.
In this paper, we present the minimal discrete match (MDM) concept.
The minimal discrete match is the smallest set of discrete particles needed to match a given GSD.
We present a method for determining the MDM for any GSD and discuss strategies for finding the smallest MDM within a set of tolerances on the GSD.
A mapping of USCS classification and MDM reveals a broad distribution of computational cost over several orders of magnitude for granular soils.

## Keywords

Discrete soil description, discrete element method, open-source algorithms, computational cost

## Introduction

```{python}
import sys
sys.path.append('..')
import math
from gsd_lib import GSD, MinimalPackingGenerator
import numpy as np
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams.update(
    {
        # Figure settings
        "figure.figsize": (5, 4),
        "figure.dpi": 400,
        "savefig.dpi": 600,
        "savefig.bbox": "tight",
        "savefig.pad_inches": 0.1,
        # Font settings
        "font.size": 12,
        "text.usetex": True,
        "font.family": "serif",
        "font.sans-serif": ["cmss10", "Arial"],
        # "font.serif": ["Latin Modern", "cmr10"],
        # "font.monospace": ["cmtt10"],
        "axes.formatter.use_mathtext": True,
        "axes.titlesize": 14,
        "axes.labelsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        "legend.fontsize": 10,
        # Axes settings
        "axes.linewidth": 1.0,
        "axes.spines.top": True,
        "axes.spines.right": True,
        "axes.grid": True,
        "grid.alpha": 0.3,
        "grid.linewidth": 0.5,
        # Line and marker settings
        "lines.linewidth": 1.5,
        "lines.markersize": 6,
        "scatter.marker": "o",
        # Legend settings
        "legend.frameon": True,
        "legend.framealpha": 0.8,
        "legend.fancybox": True,
        "legend.numpoints": 1,
        # Tick settings
        "xtick.direction": "in",
        "ytick.direction": "in",
        "xtick.major.size": 4,
        "ytick.major.size": 4,
        "xtick.minor.size": 2,
        "ytick.minor.size": 2,
    }
)


```

Soil is a fundamentally discrete granular material whose complex mechanical behavior emerges from the numerous interactions between its constituent parts.
The discrete element method (DEM), introduced by @cundall1979, has proven to be a powerful tool in exploring the inter-particle interactions of granular assemblies.
With DEM, a vast parametric space exists where grain sizes, shapes, contact models, etc. can be systematically varied and their influences on macro-scale assembly behavior can be quantified.

These models pose several challenges that must be managed in order for their benefits to be realized.
Due to the number of elements involved in typical DEM simulations, both the computational resources needed to run them and the ability to characterize and interpret them are non-trivial even at lab specimen scales.
Over the years, the number of particles used in DEM simulations has increased, but not proportionally to the reduction in costs of computational resources.
@osullivan2014 showed that the order of magnitude of average number of particles in DEM simulations increased substantially from 1998 to 2014, from approx.
$\mathcal{O}(10^3)$ to $\mathcal{O}(10^5)$, but fell far short of the $\mathcal{O}(10^7)$ predicted by @cundall2001 for "easy" geomechanics problems by 2011.
More recent publications in DEM have shown that orders ranging from $\mathcal{O}(10^6)$ (@sufian2021, @dong2022) to $\mathcal{O}(10^8)$ (@miyai2019, @fang2021, @zhang2024) are possible, but rare and require high performance computing resources.

Because of the computational cost, DEM simulations whose particles approach realistic grain sizes generally model at the representative volume (RV) element scale.
Several nuances to the RV concept exist, but broadly speaking, the RV represents the smallest volume of material to exhibit statistically consistent macro-scale behavior as its source material [@gitman2007].
In physical laboratory experiments and discrete numerical experiments, ensuring study samples are sufficiently large to behave as an RV is critical to the broader applicability of the results.

The RV size depends on several factors including the grain size distribution (GSD), sample density, loading conditions, grain shape and arrangement, and the behavior of interest.
Studies have tied RV to sample height to maximum particle diameter ratios with values between 15 and 20 needed to achieve RV [@cantor2025].
Additionally, there is evidence that the average particle size [@zeraati-shamsabadi2025] and overall polydispersity [@bandera2021] also influence RV size.
Intuitively, with increasing GSD breadth, the number of particles needed in a DEM simulation of granular assemblies will also increase.
Often, matching the true GSD of interest (e.g., from a physical soil sample) is sacrificed for computational efficiency by truncating or upscaling the distribution.

While the insights we gain from DEM simulations with upscaled or truncated GSDs are valuable, they come with acknowledged limitations on their broader applicability to natural soil fabrics.
It becomes an important experimental design consideration, then, to understand the relative computational costs associated with different grain size distributions.
For very specific GSDs, formulas can be used to calculate the number of particles needed.
For example, @tyler1992 provide such a formula for self-similar fractal distributions.
However, they note that the formula results are sensitive to the choice of representative radius for a given range.
Further, they concluded that a self-similar fractal distribution is a good model for pore structure, but not necessarily grain size.
For general GSDs, particularly when a real soil is being modeled, an ideal approach would involve an analytical solution to the minimum number of particles required to reproduce the GSD.

This paper introduces such an approach through the minimal discrete match (MDM) concept.
The MDM is the smallest set of discrete particles needed to match a given grain size distribution by mass.
Note that the MDM is based on mass-size relationships only, not mechanical behavior.
Therefore, it represents an important component to determining RV and relative computational cost, but it is not equivalent to an RV.

In order to characterize the MDM, descriptions of grain size distributions and granular samples are presented as mathematical sets and the rules describing several relationships between the sets are defined.
Using these definitions, algorithms are presented for both an approximate and a rigorous solution for the MDM, which can be projected to calculate sample-scale particle quantities.
The method is compared to published DEM models and shows a broad range of MDM magnitudes over several GSDs of interest in geotechnical engineering.

## Discrete definitions

Identifying the minimal discrete match requires rigorous mathematical definitions for particulate samples, $S$, and grain size distributions, $G$.
Conceptually, each of these are collections of items (i.e., sets) with specific restrictions.
The definitions are designed to capture the physical reality underlying a mechanical grain size analysis.

### Sample definition

A sample, $S$ is an indexed set describing its particles as pairs of size, $X_S$, and quantity, $Q_S$.
$X_S$ is an ordered (i.e., each entry is larger than the previous) set of unique values of positive real numbers.
$Q_s$ is an unordered set of positive integers related to $X_S$ by a shared indexing set for the sample, $I_S$.

$$S = \{(X_S, Q_S) \in I_S\}$$ {#eq-sample_def} where:

$$X_S = \{x_i : i \in I_S\} \text{ with } x_1 < x_2 < \cdots < x_{n_S}$$ {#eq-size_sample}

$$
Q_S = \{q_i : i \in I_S\}
$$ {#eq-quantity_sample}

$$
X_S \subset \mathbb{R}^+ \text{ and } Q_S \subset \mathbb{Z}^+
$$ {#eq-domains_sample}

### Grain size distribution definition

A grain size distribution, $G$, has a similar structure.
It is and indexed set describing size boundaries, $X_G$, and masses, $M_G$.
Like $X_S$, $X_G$ is an ordered set of unique values, however, it's domain also includes zero (representing the pan in a typical sieve analysis).
Like $Q_S$, $M_G$ shares an index ($I_G$) with the size set, but it's domain is not restricted to integers, only to non-negative real numbers.
The values in $M_G$ represent the masses retained on a sieve with opening size $X_G$.

$$G = \{(X_G, M_G) \in I_G\}$$ {#eq-gsd_def} where:

$$
X_G = \{x_j : j \in I_G\} \text{ where } x_1 < x_2 < \cdots < x_{n_G}
$$ {#eq-size_gsd}

$$
M_G = \{m_j : j \in I_G\}
$$ {#eq-mass_gsd}

$$
X_G \subset \mathbb{R}^{non-neg} \text{ ; } M_G \subset \mathbb{R}^{non-neg}
$$ {#eq-domains_gsd}

### Comparison definition

The relationship between $S$ and $G$ cad be defined in terms of how $G$ *describes* $S$.
In a later section, the topic of finding an instance of $S$ that *matches* a target $G$ will be addressed.

The relationship between $S$ and $G$ is formalized with the following conditions:

**Condition 1:** $G$ is complete if and only if the final entry in $M_G$ is zero: $$G \text{ is complete } \Leftrightarrow m_{n_G} = 0$$ {#eq-gsd_complete}

This condition ensures that $x_{nG}$ provides an upper bound to the sizes described by $G$.
This is analogous to the limitation of scope in ASTM D6913 to grain sizes passing the 75-mm sieve, ensuring the cumulative GSD always reaches 100 percent at a known size [@d18committee].

**Condition 2:** $G$ describes $S$ (denoted $G \longrightarrow S$) if and only if $\text{Cond1}$ is met and the its smallest size is smaller than any size in $S$ and its largest size is larger than any size in $S$:

$$
G \longrightarrow S \Leftrightarrow \text{ Cond1} \land \min(X_G) < \min(X_S) \land \max(X_G) \ge \max(X_S)
$$ {#eq-gds_describes}

**Condition 3:** $G$ describes $S$ articulately if $\text{Cond2}$ is met and for all consecutive pairs of sizes in $X_S$ there exists at least one size in $X_G$ between them.

$$
G \stackrel{\text{articulately}}{\longrightarrow} S \Leftrightarrow \text{Cond2} \land \forall (x_i, x_{i+1}) \in X_S \quad \exists\, x_j \in X_G \text{ such that } x_i < x_j \le x_{i+1}
$$ {#eq-gsd_articulate}

**Condition 4:** $G$ describes $S$ accurately if $\text{Cond2}$ is met and the combined masses of all the particles with sizes between every pair of sizes in $X_G$ is equal to the retained mass on the lower of the size pairs in $G$:

$$
G \stackrel{\text{accurately}}{\longrightarrow} S \Leftrightarrow \text{ Cond2} \land \sum_{\substack{i \in I_S \\ x_j < x_i \le x_{j+1}}} q_i \cdot f(x_i) = m_j
$$ {#eq-gsd_accurate}

where $f(x)$ is a scaling function that converts size to mass.

$\text{Cond4}$ represents what is generally meant by a "match" between a sample and a grain size distribution.
Note that $\text{Cond3}$ is not required for $\text{Cond4}$ to be satisfied.

**Match error:** The error for a match between $G$ and $S$ is simply a measure of how far $\text{Cond4}$ is from being met:

$$
\text{Error}(G, S) = m_j \quad - \sum_{\substack{i \in I_S \\ x_j < x_i \le x_{j+1}}} q_i \cdot f(x_i)
$$ {#eq-error}

### Physical interpretation

Some assumptions are required in order for $S$ and $G$ to be interpreted as a physical sample and sieve analysis (or a numerical version of the same).
First, the sizes in $X_G$ are assumed to represent smallest enclosing diameter of whatever shapes the particles in $S$ are.
Second, as indicated in @eq-gsd_accurate, a size in $X_S$ is considered between two sizes in $X_G$ if it is larger than the $x_j$ and smaller than or equal to $x_{j+1}$.
In other words, particles pass through openings equal to their own size.
Finally, the particles in $S$ are assumed to be of similar enough shape that a single scaling function, $f(x)$, can represent the size-mass relationship for all of $S$.
For spherical particles with of constant density ($\rho$), this is a straightforward definition:

$$f(x) = \frac{\rho\pi}{6}x^3$$ {#eq-mass_scaling}

Note, however, that the use of this particular scaling function is not required in general.
It is only necessary that $f(x)$ be some mapping of $x \mapsto  m$.
It would be possible to combine sets of differently shaped particles of different densities with respective mapping functions, but this paper will limit itself to assuming uniform, spherical particles.

## GSD study suite

This paper will analyze several grain size distributions in the range of interest in geotechnical engineering (although the methods are applicable over a wider range).
Specifically a suite of 2750 randomized GSDs based on the sieve set specified in ASTM D6913 (14 opening sizes from 0.075 to 75 mm) are analyzed.
The distributions were generated by adding random noise to normal mass distributions around a "central" size.
The central size itself was randomly selected from the middle half of the sieves used.
Between 5 and 14 consecutive sieves were used in each GSD.
For classification purposes, articles smaller than 0.075 are assumed to be low plasticity silt, regardless of their size.

@tbl-gsd_params summarizes the GSD suite using several traditional characteristics including coefficient of curvature, $C_c$, and coefficient of uniformity, $C_u$.
The grain size index, $I_{GS}$, as defined by @erguler2016 and a new parameter, the curvature index, $I_C$, are also presented.
The curvature index is conceptually similar to the grain size index in that it is a ratio of areas related to the cumulative distribution function of the GSD.
$I_C$ normalizes the area under the GSD curve to the trapezoidal area bounded by the percent passing the smallest size above the pan ($x_{min+1}$) and the largest size ($x_{max}$).
@fig-gsd_curvature_index shows a subset (about 20%) of the GSD suite and illustrates the definition of $I_C$ on an example GSD.

::: {#tbl-gsd_params}
| Parameter        | Value(s) in study |
|------------------|-------------------|
| Instances of $G$ | 2750              |
| Sieves in $G$    | 5 to 14           |
| $x_{min+1}$      | 0.075 to 25 mm    |
| $x_{max}$        | 9.5 to 75 mm      |
| Percent fines    | 0.4% to 34%       |
| $C_c$            | 0.2 to 25.5       |
| $C_u$            | 1.2 to 214.5      |
| $I_{GS}$         | 0.03 to 0.60      |
| $I_C$            | 0.29 to 1.57      |

Grain size distribution parameters in this study
:::

```{python}
# Setup for generating example grain size distributions

## Standard U.S. sieve sizes (mm)
x1 = np.array(
    [
        0.0001,  # Pan
        0.075,  # Number 200 sieve separates coarse from fines
        0.15,
        0.3,
        0.6,
        1.18,
        2.36,
        4.75,  # Number 4 sieve separates sand from gravel
        9.5,
        19,
        25,
        37.5,
        50,
        63,
        75,  # 3-inch sieve (100% passing this per ASTM D2487-17 1.2)

    ]
)

sieve_sizes = x1.copy()
n_sieves = len(sieve_sizes)

## Generate random grain size distributions on the sieve set
def mass_dist(n_sieves, rng=None, exponent=4.0, extra_randomness = 0):
    if rng is None:
        rng = np.random.default_rng()
    base_dist = 1*np.ones(n_sieves)  + rng.random(n_sieves)
    lower = 1 + n_sieves // 4
    upper = 1 + 3 * n_sieves // 4
    center_idx = rng.integers(lower, upper)
    distances = np.abs(np.arange(n_sieves) - center_idx) / center_idx
    scale_factor = np.exp(-exponent * distances)
    dist = base_dist * scale_factor
    for i in range(extra_randomness):
        dist *= 1 + rng.random(n_sieves)   # Add some extra randomness
    return dist
  
## A random generator with a fixed seed for reproducibility
rng = np.random.default_rng(1)

tol = 1e-2

gsd_list = []
set_size_list = []
flex_list = []

## Generate a small batch of GSDs
x = x1.copy()
min_span = 5
for start in range(0, len(x) - min_span):
    for end in range(start + min_span, len(x)):
        new_x = x[start:end]
        n_sieves = len(new_x)

        for j in range(10):
            # Create a distributed set of retained masses
            mass = mass_dist(n_sieves, rng, exponent=2, extra_randomness=7)

            mass[-1] = 0.0  # Ensure last mass is zero
            g = GSD(sizes=new_x, masses=mass)

            # Create a flexible minimal packing for each GSD
            mps_f = MinimalPackingGenerator(
                g, x_n_factor=0.5, tol=tol, flex=True, density=1.0
            )

            # Create a set size minimal packing for each GSD
            mps_s = MinimalPackingGenerator(
                g, x_n_factor=0.5, tol=tol, flex=False, density=1.0
            )
            gsd_list.append(g)
            set_size_list.append(mps_s)
            flex_list.append(mps_f)
```

```{python}
#| label: fig-gsd_curvature_index
#| fig-cap: "A representative subset (~10%) of the GSD suite. The area ratio defining the curvature index is shown. The curve in the highlighted example has a curvature index of 1.09."

    
# grain size distribution plot
# plt.close('all')
fig, ax = plt.subplots()
for gsd in gsd_list:
    ax.plot(gsd.sizes[1:], 100*gsd.percent_passing[1:], color='k', alpha=0.2, linewidth=0.5)

ex_i = 39
example_gsd = gsd_list[ex_i]
ax.plot(example_gsd.sizes[1:], 100*example_gsd.percent_passing[1:], color='r', linewidth=2, label='Example GSD')
ax.plot(
    [
        example_gsd.sizes[1],
        example_gsd.sizes[1],
        example_gsd.sizes[-1],
        example_gsd.sizes[-1],
    ],
    [
        100 * example_gsd.percent_passing[1],
        0.5,
        0.5,
        100 * example_gsd.percent_passing[-1],
    ],
    color="r",
    linewidth=2,
    label="Example Retained",
)

ax.plot(
    [
        0.075,
        example_gsd.sizes[-1],
        example_gsd.sizes[-1],
        0.075,
        0.075,
        example_gsd.sizes[-1],
    ],
    [0.5, 0.5, 100, 100 * example_gsd.percent_passing[1], 0, 0],
    color="b",
    linestyle=":",
    linewidth=2,
    label="Uniform Distribution",
)

ax.plot([11,40], [52,52], color='k', linewidth=1,zorder=4)
left, bottom, width, height = (5.5, 40, 80, 30)
rect = plt.Rectangle((left, bottom), width, height, fill=True, color="1", alpha=0.8,zorder=3)

arrowprops = dict(arrowstyle="->", color="k", lw=1)
bbox = dict(fc="1", ec="1", alpha=0.9)


ax.add_patch(rect)
ax.annotate(
    r"\textbf{Curvature Index:}",
    xy=(5, 65),
    xytext=(6.5, 65),
    fontsize=11,
    # bbox=bbox,
    color="k",
    ha="left",
    va="center",
)
bbox = dict(fc="1", ec="r", linewidth=2, alpha=1)
ax.annotate(
    r"Area",
    xy=(5, 65),
    xytext=(20, 58),
    fontsize=11,
    bbox=bbox,
    color="k",
    ha="center",
    va="center",
)
bbox = dict(fc="1", ec="b", linestyle=":", linewidth=2, alpha=1)
ax.annotate(
    r"Area",
    xy=(5, 65),
    xytext=(20, 46),
    fontsize=11,
    bbox=bbox,
    color="k",
    ha="center",
    va="center",
)

ax.set_xscale('log')
ax.grid()#(True, which='both', axis='both')
ax.set_xlabel('Particle Size (mm)')
ax.set_ylabel(r'Percent Passing (\%)')
ax.set_ylim(0, 100)
ax.set_xlim(0.055, 100)
```

## Minimal discrete match solution

This section presents the use of the discrete definitions in the previous section in the formulation of the solution to the MDM problem.
The solution takes the form of the MDM sample, $S_{MDM}$, which contains the the minimum number of particles ($N_{MDM}$) needed for a mass-based match to $G$.
The solution is then projected to the simulation scale, allowing for an estimate of the number of particles in an simulation $N_{sim}$ of arbitrary size.

### Mass-based matching

As a starting point, assuming that $G$ is an articulate description of $S$ limits the number of sizes in $S$ to $n_G-1$.
Temporarily setting aside the minimization goal, any valid sizes may be selected for $X_S$.
From $G$, an indexed set of mass ratios, $\Phi$, can be defined to describe the masses of each size range relative to the mass retained on the second to largest sieve (recall that no mass is retained on the largest sieve):

$$\Phi = \left\{\frac{m_j}{m_{n_G-1}} : j \in I_G-1\right\} \text{; with elements } \phi_j$$ {#eq-mass_ratio}

The length of $\Phi$ is one less than $I_G$, making it equal in length to $I_S$ because of $\text{Cond3}$.

Similarly, a set of volume ratios, $Z$ can be defined to describe the volume per particle relative to the largest particle:

$$Z= \left\{\frac{f(x_{nS})}{f(x_i)} : i \in I_S\right\} \text{; with elements } \zeta_i$$ {#eq-vol_ratio}

Note that $Z$ is referred to as a volume ratio to avoid confusion with $\Phi$ despite the fact that the mapping function $f(x)$ converts size to mass.
Interpreting $Z$ as a volume ratio is valid under a constant density assumption.

Because they are ratios relative to the largest considered size, $\phi_{nS} =\zeta_{nS} = 1$.
Since $\Phi$ describes relative masses of each size and $Z$ describes the relative volume (and mass) per particle of each size, their product will describe a relative quantity ratio, $K$:

$$K= \left\{\phi_i \times \zeta_{i} : i \in I_S\right\} \text{; with elements } \kappa_i$$ {#eq-quant_ratio}

The quantity ratio $K$ is the relative number of particles of each of the assumed sizes in $X_S$ needed to satisfy $\text{Cond4}$.
It is directly proportional to the key target parameter, $Q_S$.
Being the product of $\phi_{nS}$ and $\zeta_{nS}$, $\kappa_{nS}$ will be an integer equal to 1, making it the smallest allowable candidate for $q_{nS}$.
Unfortunately, with the exception of $\kappa_{nS}$, the entries in $K$ are not guaranteed to be integers.
Rounding each element in $K$ to the nearest integer provides a first-order, approximate solution for the minimal discrete match.

$$
S_{approx} = \{(X, \text{int}(K)) \in I_S\} \approx S_{min}
$$ {#eq-s_approx}

Depending on the application and acceptable error tolerance, $S_{approx}$ may be a suitable substitute for $S_{min}$.
The error for the approximate solution that assumed fixed sizes can be reduced using the following fixed size ($FS$) algorithm:

1.  Find $K$ using @eq-quant_ratio and define an integer value $i=0$.
2.  Use $\text{int}((i+1) \times K)$ to approximate $Q_{S(i)}$ and build $S_{(i)} = \{ X_S, Q_{S(i)} \}$.
3.  Calculate the error, $E_{(i)}$, between $G$ and $S_{(i)}$ with @eq-error.
4.  If $E_{(i)}$ exceeds the desired tolerance, increase $i$ by $1$.
5.  Repeat Steps $FS_2$ through $FS_4$ until the desired tolerance is reached.

This approach can reduce the match error to an arbitrarily low value, but the total number of particles grows by a factor of $|Q_{S(0)}|$ with each iteration.

But since the sizes in $X_S$ are only bounded by $X_G$ and not explicitly defined, the opportunity exists to find a better selection of entries in $X_S$ that will minimize $Q_S$.
The best allowable sizes for $X_S$ can be found using the following spanned integer ($SI$) algorithm:

1)  Find the quantity ratios associated with the minimum ($-$) and maximum ($+$) allowable sizes in $x_1$ through $x_{nS-1}$ (i.e. $K_-$ and $K_+$) and define an integer value $i=0$.
2)  Check whether an integer is spanned between each entry in $K_-$ and $K_+$. An integer is spanned between two consecutive numbers $a$ and $b$ if $\lceil a \rceil \leq \lfloor b \rfloor$ (i.e., $a$ is rounded to the next highest integer and $b$ is rounded to the next lowest integer).
3)  If a spanned integer ($SI$) does not exist for each entry, repeat Steps $SI_1$ and $SI_2$ with incrementally larger sizes for $x_{nS}$.
4)  If, after reaching the maximum allowable particle size for $x_{nS}$, an $SI$ does not exist between each entry in $K_-$ and $K_+$, increase $i$ by $1$ and perform Step $FS_2$ (previous algorithm) and repeat Steps $SI_1$ to $SI_3$ (this algorithm).
5)  When an integer quantity exists between each entry in $K_-$ and $K_+$, these integers can be used to populate the quantity set $Q_{SI}$.
6)  Invert the mass scaling function $f(x)$ on the ratio $\Phi$ / $Q_{SI}$ to identify the compatible sizes for the spanned integer quantities:

$$
X_{SI} = f^{-1} \left( \frac{\Phi}{Q_{SI}} \right) 
$$ {#eq-spanned_int_sizes}

Because $Q_{SI}$ was generated using the allowable sizes of $X_S$ and the mass ratios of the target $G$, the resulting $X_{SI}$ satisfies $\text{Cond4}$ with $Q_{SI}$ as the smallest possible quantity of particles, or the minimal discrete match:

$$
MDM = S_{min} = \{X_{SI}, Q_{SI}\}
$$ {#eq-mdm}

The total number of particles in the MDM is the sum of the quantities in $Q_{SI}$:

$$
N_{MDM} = \sum_{i \in I_S} q_{SI,i}
$$ {#eq-n_mdm}

The effectiveness of the spanned integer algorithm is shown in the rapid convergence to numeric zero error compared to a fixed-size approximation in @fig-convergence.
The figure shows both approaches attempting to find a suitable sample for each of the GSDs shown in @fig-gsd_curvature_index with an allowable total error tolerance of 1 percent.
In nearly all cases (over 98% for the GSDs in this study), the spanned integer approach requires no quantity-increasing iteration (i.e., Step $SI_4$ is skipped).
Where iteration is needed, it converges to numerical zero within one or two iterations.
The fixed size approach converges with between 1 and 15 iterations and retains measurable error.
This difference is significant, not because the spanned integer algorithm itself is computationally expensive, but because each iteration represents an increase in total particles in the final discrete sample.

```{python}
#| label: fig-convergence
#| fig-cap: "Packing Algorithm Convergence"

# Packing algorithm convergence plot
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, height_ratios=[4, 1])
# fig, ax1 = plt.subplots()
j = 1
nf = []
ns = []

for i in range(len(set_size_list)):
    # f = flex_list[i]
    s = set_size_list[i]
    # xf = [n[-1] for n in f.qs]
    xs = [n[-1] - 1 for n in s.qs]
    # f_error = f.error
    s_error = s.error

    # nf.append(xf[-1])
    ns.append(xs[-1])
    # ax1.plot(xf, f.error, color="k", alpha=1)
    # ax2.plot(xf, f.error, color="k", alpha=0.5)
    ax1.plot(xs, s.error, color="r", alpha=0.5)

for i in range(len(flex_list)):
    f = flex_list[i]
    # s = set_size_list[i]
    xf = [n[-1] - 1 for n in f.qs]
    # xs = [n[-1] for n in s.qs]
    f_error = f.error
    # s_error = s.error

    nf.append(xf[-1])
    # ns.append(xs[-1])
    ax1.plot(xf, f.error, color='k', alpha=0.5)
    ax2.plot(xf, f.error, color="k", alpha=0.5)
    ax2.scatter(xf, f.error, marker='o',color='k', alpha=0.5)
    # ax1.plot(xs,s.error,color="r", alpha=1)


ax1.plot([0, 200], [tol, tol], color='k', linestyle='--', linewidth=1)



# # Calculate histograms
# counts_ns, bins_ns = np.histogram(ns, bins=50)
# counts_nf, bins_nf = np.histogram(nf, bins=bins_ns)

# # Normalize to max frequency
# norm_counts_ns = counts_ns / np.max(counts_ns)
# norm_counts_nf = counts_nf / np.max(counts_nf)

# # Plot normalized histograms
# ax2.bar(
#     bins_ns[:-1],
#     norm_counts_ns,
#     width=np.diff(bins_ns),
#     alpha=1,
#     label="Fixed Size",
#     color="red",
# )
# ax2.bar(
#     bins_nf[:-1],
#     norm_counts_nf,
#     width=np.diff(bins_nf),
#     alpha=1,
#     label="Spanning Integer",
#     color="black",
# )


ax1.set_yscale("log")
# ax1.set_xscale("log")
ax1.set_ylim(1e-4, 5e-1)
ax1.set_xlim(0, 16)
# ax1.set_xscale("log")
# ax1.set_title("Packing Set Convergence")

ax2.set_yscale("log")
# ax1.set_xscale("log")
ax2.set_ylim(5e-18, 5e-16)

ax2.set_xlabel(r"Iterations")
ax1.set_ylabel("Discrete Match Error")

arrowprops = dict(arrowstyle="->", color="k", lw=1)
bbox = dict(fc="1",ec="1")

ax1.annotate(
    r"$\epsilon_{tol}$",
    xy=(10, tol),
    xytext=(12, 1e-3),
    fontsize=14,
    arrowprops=arrowprops,
    bbox=bbox,
    color="k",
    ha="left",
    va="center",
)

ax2.annotate(
    "If any iteration needed,\nSI converges to numeric zero quickly.",
    xy=(2.5, 1e-16),
    xytext=(3.5, 5e-17),
    fontsize=12,
    arrowprops=arrowprops,
    bbox=bbox,
    color="k",
    ha="left",
    va="center",
)

from matplotlib.lines import Line2D

custom_lines = [
    Line2D([0], [0], color="red", lw=2, alpha=0.5, label="Fixed Size"),
    Line2D([0], [0], color="black", lw=2, alpha=0.5, label="Spanned Integer"),
]

# fig, ax = plt.subplots()
# lines = ax.plot(data)
ax1.legend(title="MDM Algorithm", handles=custom_lines, loc='upper right')
```

In some sense, the $FS$ approach would appear invalid since it tends to find larger "minimal" solutions than the $SI$ approach, but there are scenarios in which it can be useful.
For example, in cases where specific particle sizes (as opposed to ranges) are needed, as is the case with space-filling problems [@botet2021].
In the context of mass-distribution fitting, the $FS$ results show how sensitive the minimal discrete matching problem is to the selection of sizes.
The $SI$ algorithm finds the ideal sizes where the relative particle quantities fall neatly into place to match the target GSD.
Even over the fairly small available size ranges, the $FS$ algorithm performance shows that deviation from these ideal sizes can carry a significant penalty.
The results in the rest of this paper use the spanned integer algorithm.

### Projection to simulation scale

The volume of the MDM is not the same as the RV or any specific physical sample size.
But converting the MDM results to a simulation scale is straightforward if the volume and target void ratio are known.
The number of particles in a simulation-scale sample ($N_{sim}$) can be estimated using:

$$
N_{sim} = N_{MDM} \times \frac{V_{sim}}{V_{MDM} \times (1 + e)}
$$ {#eq-sample_scale}

$V_{MDM}$ is the volume of solids in the MDM, which can be calculated from the total MDM mass (@eq-gsd_accurate) and particle density.
The result of @eq-sample_scale should be considered an estimate of the total particles of the full-scale DEM as it does not take into account boundary effects or the particle-generating algorithms used by DEM codes.
Also note that whether the simulation volume, $V_{sim}$, meets or is intended to meet the criteria for RV is the modeler's responsibility and outside the scope of this work.

## Results

```{python}

# Data from Zeraati and Shamsabadi (2025)
zs_data = {
    "D": [
        50,
        70,
        105,
        175,
        50,
        70,
        105,
        175,
        70,
        105,
        175,
        70,
        105,
        175,
    ],
    "SF": [5, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 20, 20, 20],
    "H": [
        24.4,
        24.3,
        23.6,
        23.6,
        24.8,
        25.2,
        24.4,
        24.4,
        25.6,
        24.6,
        24.1,
        26.0,
        24.8,
        25.0,
    ],
    "e_c": [
        0.683,
        0.684,
        0.669,
        0.699,
        0.714,
        0.645,
        0.678,
        0.672,
        0.754,
        0.645,
        0.644,
        0.800,
        0.741,
        0.748,
    ],
    "N": [
        103075,
        202972,
        447161,
        1255067,
        12408,
        24349,
        54564,
        155028,
        7185,
        15719,
        44522,
        2816,
        6667,
        18180,
    ],
}


```

This section presents the results of a comparison between the MDM methods prediction of $N_{sim}$ and an independent DEM study as well as the results for $N_{MDM}$ for the GSD suite described in @tbl-gsd_params.

### Comparison to independent DEM data

The MDM-based simulation-scale estimate method was tested against the DEM models from a study on upscaled GSDs of Athabasca oil tailings sand by @zeraati-shamsabadi2025.
The study upscaled the actual sand GSD (for computational efficiency) with a scale factor ($SF$) ranging from 5 to 20 and produced simulation samples with diameters ranging from 50 to 175 mm with consolidated sample heights and void ratios from 23.6 to 26.0 mm and 0.64 to 0.8, respectively.
According to the USCS, unscaled Athabasca sand and the GSDs up to $SF$ of 15 are classified as "poorly graded sand"; the coarsest model ($SF=20$) is "poorly graded sand with gravel."

@fig-demo shows a strong agreement between of the total number of simulation particles predicted by the MDM method and those reported in the @zeraati-shamsabadi2025 study.
Relative to the predicted values, the study reported a moderately larger number of particles for all the samples.
This suggests the DEM models used different discrete representation of the scaled GSDs than the MDM, which is expected.
Even so, the consistent trend confirms the MDM captures the relative computational efforts and provides a good estimate of the actual $N_{sim}$ independent of the specific discrete GSD match.
Figure also shows the MDM methods ability to predict the particles needed for extremely high resolution DEM models without generating them.
Relative to the highest resolution models in the study ($SF=5$), the MDM method indicates simulations of unscaled Athabasca sand would require roughly $10^2$ times the number of pa

```{python}
df = pd.DataFrame(zs_data)
df["sample_volume"] = np.pi * (df["D"] / 2) ** 2 * df["H"]
# df

# athabasca_sizes = np.array(
#     [
#         0.07647757992149977,
#         0.10668723256342968,
#         0.15160826983319625,
#         0.2521194933444675,
#         0.4270926409538863,
#         0.854529200202823,
#         2.36316461948315,
#     ]
# )
athabasca_sizes = np.array([0.076, 0.11, 0.15, 0.25, 0.43, 0.85, 2.40])
# athabasca_sizes = np.array([0.076, 0.11, 0.15, 0.25, 0.43, 0.85, 2.40, 4.50])
athabasca_sizes = np.insert(athabasca_sizes, 0, athabasca_sizes[0] / 2)
# athabasca_percent_passing = np.array([3.9, 5.14, 15.27, 60.13, 81.35, 93.40, 99.5, 100.00])
athabasca_percent_passing = np.array([3.86, 5.14, 15.27, 60.13, 81.35, 93.40, 100])
# athabasca_percent_passing = np.array(
#     [
#         3.879310344827573,
#         5.02873563218391,
#         15.229885057471265,
#         60.201149425287355,
#         81.4655172413793,
#         93.39080459770115,
#         99.56896551724138,
#     ]
# )
athabasca_percent_passing = np.insert(athabasca_percent_passing, 0, 0.0)
athabasca_retained = np.zeros(len(athabasca_sizes))
for i in range(len(athabasca_sizes) - 1):
    athabasca_retained[i] = (
        athabasca_percent_passing[i + 1] - athabasca_percent_passing[i]
    )
# athabasca_retained = vround(athabasca_retained, 2)

df["G"] = [
    GSD(sizes=athabasca_sizes * sf, masses=athabasca_retained) for sf in df["SF"]
]
df["MDM"] = [
    MinimalPackingGenerator(gsd, x_n_factor=0.5, tol=1e-2, flex=True).mps
    for gsd in df["G"]
]
df["N_mdm"] = [sum(mdm.quantities) for mdm in df["MDM"]]
df["V_solids_mdm"] = [mdm.total_volume for mdm in df["MDM"]]
df["V_total_mdm"] = df["V_solids_mdm"] * (1 + df["e_c"])
df["MDM_per_sample"] = df["sample_volume"] / df["V_total_mdm"]
df["N_predicted"] = df["N_mdm"] * df["MDM_per_sample"]
df["USCS_name"] = [g.uscs_name for g in df["G"]]

unscaled_df = pd.DataFrame({"D": [50, 70, 105, 175]})
unscaled_df["G"] = [
    GSD(sizes=athabasca_sizes, masses=athabasca_retained) for _ in range(4)
]
unscaled_df["MDM"] = [
    MinimalPackingGenerator(gsd, x_n_factor=0.5, tol=1e-2, flex=True).mps
    for gsd in unscaled_df["G"]
]
unscaled_df["N_mdm"] = [sum(mdm.quantities) for mdm in unscaled_df["MDM"]]
unscaled_df["V_solids_mdm"] = [mdm.total_volume for mdm in unscaled_df["MDM"]]
unscaled_df["V_total_mdm"] = unscaled_df["V_solids_mdm"] * (1 + 0.67)
unscaled_df["MDM_per_sample"] = (
    25 * np.pi * (unscaled_df["D"] / 2) ** 2
) / unscaled_df["V_total_mdm"]
unscaled_df["N_predicted"] = unscaled_df["N_mdm"] * unscaled_df["MDM_per_sample"]
unscaled_df["USCS_name"] = [g.uscs_name for g in unscaled_df["G"]]

```

```{python}
#| label: fig-demo
#| fig-cap: "Comparison of MDM-predicted sample sizes for scaled GSDs in Z study. Predictions for unscaled models of A sand."
plt.close("all")
fig, ax = plt.subplots()

markers = ["v", "s", "D", "^"]
colors = plt.cm.viridis(
    np.linspace(0, 1, len(df["D"].unique()))
)  # Color map for D values

# Create a mapping for SF values to markers
sf_values = sorted(df["SF"].unique())
sf_to_marker = {sf: markers[i % len(markers)] for i, sf in enumerate(sf_values)}
# Add SF=1 for unscaled data
sf_to_marker[1] = "o"  # Use 'x' marker for SF=1 (unscaled)

# Create a mapping for D values to colors
d_values = sorted(df["D"].unique())
d_to_color = {d: colors[i] for i, d in enumerate(d_values)}

# Plot scaled data
for _, row in df.iterrows():
    ax.scatter(
        row["N_predicted"],
        row["N"],
        s=50,
        marker=sf_to_marker[row["SF"]],
        color=d_to_color[row["D"]],
        alpha=1,
        edgecolors="black",
        linewidth=0.5,
    )

# Plot unscaled data with same color scheme
unscaled_x = []
unscaled_y = []
for _, row in unscaled_df.iterrows():
    ax.scatter(
        row["N_predicted"],
        row["N_predicted"],  # or use actual N if available
        s=100,
        marker=sf_to_marker[1],  # Use SF=1 marker
        color=d_to_color[row["D"]],
        alpha=1,
        edgecolors="black",
        linewidth=0.5,
        zorder=5,
    )
    unscaled_x.append(row["N_predicted"])
    unscaled_y.append(row["N_predicted"])

# Add rectangle around prediction
from matplotlib.patches import FancyBboxPatch

# Create rectangle around prediction
prediction_box = FancyBboxPatch(
    (8e3, 5e6),
    width=2e8,
    height=2e8,
    boxstyle="round",
    # linewidth=2,
    edgecolor="k",
    facecolor="none",
    # linestyle="--",
    # alpha=0.8,
)

ax.add_patch(prediction_box)

# Create rectangle around data
data_box = FancyBboxPatch(
    (1.2e3, 1.2e3),
    width=2e6,
    height=2e6,
    boxstyle="round",
    # linewidth=2,
    edgecolor="k",
    facecolor="none",
    # linestyle="--",
    # alpha=0.8,
)

ax.add_patch(data_box)

# ...rest of your existing code...
arrowprops = dict(arrowstyle="->", color="k", lw=1)
bbox = dict(fc="1", ec="1")

# ax.annotate(
#     "MDM predictions for\nunscaled models",
#     xy=(5.4e6, 2.8e7),
#     xytext=(1e4, 9e7),
#     fontsize=10,
#     arrowprops=arrowprops,
#     bbox=bbox,
#     color="k",
#     ha="left",
#     va="center",
# )

ax.text(
    x=1e4,
    y=1e7,
    s="MDM predictions for N\nrequired to build\nunscaled DEM models\nof Athabasca sand",
    fontsize=12,
)

ax.text(
    x=1.3e3,
    y=1.1e5,
    s="Zeraati-Shamsabadi \nand Sadrekarimi\n(2025) scaled\nDEM data",
    fontsize=12
)

# Create custom legend for markers (SF values including unscaled)
all_sf_values = sorted(list(sf_values) + [1])  # Include SF=1
marker_legend_elements = [
    plt.scatter(
        [],
        [],
        marker=sf_to_marker[sf],
        color="gray",
        s=50,
        label=f"SF = {sf}" if sf != 1 else "Unscaled",
    )
    for sf in all_sf_values
]

# Create custom legend for colors (D values)
color_legend_elements = [
    plt.scatter([], [], marker="o", color=d_to_color[d], s=50, label=f"D = {d}")
    for d in d_values
]

# Add y=x line
ax.plot([1e3, 5e8], [1e3, 5e8], color="k", linestyle="--", linewidth=1)

# Add legends
legend1 = ax.legend(
    handles=marker_legend_elements,
    title="Scale Factor",
    loc="upper left",
    bbox_to_anchor=(0.62, 0.66),
    framealpha=1,
    edgecolor="1",
    fontsize="x-small"
)
legend2 = ax.legend(
    handles=color_legend_elements,
    title="Sample D (mm)",
    loc="upper left",
    bbox_to_anchor=(0.57, 0.33),
    framealpha=1,
    edgecolor="1",
    fontsize="x-small",
)
ax.add_artist(legend1)  # Keep both legends

ax.set_xscale("log")
ax.set_yscale("log")
ax.set_xlim([1e3, 5e8])
ax.set_ylim([1e3, 5e8])
ax.set_aspect("equal")  # Ensure equal scaling of axes
ax.set_xlabel(r"$N_{sim}$ (MDM predicted)")
ax.set_ylabel(r"$N_{sim}$ (reported)")
ax.grid(False)

```

### MDMs in the GSD suite

The minimal discrete matches for the GSD suite described in @tbl-gsd_params range in magnitude from $\mathcal{O}(10^1)$ to $\mathcal{O}(10^{10})$ particles.
As intuition suggests, increasing the percent mass of the smallest particle sizes or the ratio of the largest to the smallest particle sizes both tend to increase $N_{MDM}$.
@fig-phi_n illustrates a scattered linear relationship between $\phi_1$ (subscript 1 indicates the smallest particle size) and $N_{MDM}$ in log-log space for constant volume ratio, $\zeta_1$.
For $\phi_1 \approx 0.4$, $\log_{10}\zeta_1$ correlates roughly 1:1 with $N_{MDM}$.
The color bands of $\log_{10} \zeta_1$ indicate distinct combinations of min and max sieve sizes in the target $G$.

```{python}
# Create a larger dataset for plotting trends in MPS for a broad spectrum of GSDs
## Set up data structure to store GSDs and MPSs
def add_data_row(data_rows, mpgen: MinimalPackingGenerator):
    """
    Adds a row to the DataFrame with the provided keyword arguments.
    Any missing columns will be filled with NaN.

    """
    sample = mpgen.mps
    total_particles = sum(sample.quantities)
    mass_max = sample.total_masses[-1]
    mass_mid = sample.total_masses[len(sample.total_masses) // 2]
    mass_min = sample.total_masses[0]
    mass_ratio = mass_min / mass_max
    percent_fines = mass_min / sum(sample.total_masses)

    size_max = sample.sizes[-1]
    size_min = sample.sizes[0]
    size_ratio = int(np.round(size_max / size_min, 0))

    row = {
        "GSD": mpgen.g,
        "n_sieves": len(sample.sizes),
        "mass_ratio": mass_ratio,
        "size_ratio": size_ratio,
        "vol_ratio": size_ratio**3,
        "percent_fines": percent_fines,
        "total_particles": total_particles,
        "d_10": mpgen.g.d_10,
        "d_30": mpgen.g.d_30,
        "d_60": mpgen.g.d_60,
        "cc": np.round(mpgen.g.cc, 3),
        "cu": np.round(mpgen.g.cu, 3),
        "gs_index": mpgen.g.gs_index,
        "curvature_index": mpgen.g.curvature_index,
        "log_size": mpgen.g._i_gs_curve()[0],
        "gsd_curve": mpgen.g._i_gs_curve()[1],
        "slope": mpgen.g.slope,
        "curvature": np.mean(mpgen.g.curvature),  # + size_ratio**3,
        # Best so far:
        "shape_factor": np.log10(
            size_ratio**3 * (1 + np.mean(mpgen.g.curvature))
        ),  # * np.log10(size_ratio)**2),
        "asdf": mass_ratio * size_ratio**3,
        "group_symbol": mpgen.g.uscs_symbol,
        "group_name": mpgen.g.uscs_name,
    }
    data_rows.append(row)
    pass
  
## Populate dataset
data_rows = []
x = x1
min_span = 5
for start in range(0, len(x) - min_span):
    for end in range(start + min_span, len(x)):
        new_x = x[start:end]
        n_sieves = len(new_x)

        for j in range(50):
            # Create a distributed set of retained masses
            mass = mass_dist(n_sieves, rng, exponent=2, extra_randomness=7)

            mass[-1] = 0.0  # Ensure last mass is zero
            g = GSD(sizes=new_x, masses=mass)

            # Create a flexible minimal packing for each GSD
            mps = MinimalPackingGenerator(
                g, x_n_factor=0.001, tol=tol, flex=True, density=1.0
            )
            add_data_row(data_rows, mps)
            
## Convert the list of dictionaries to a DataFrame
df = pd.DataFrame(data_rows)

###
```

```{python}
#| label: fig-phi_n
#| fig-cap: "Total particles in MDM with mass ratio. Color scale indicates volume ratio."

color_param = "vol_ratio"
x_param = "mass_ratio"  # "curvature_index"
y_param = "total_particles"

fig, ax = plt.subplots()
ax.scatter(
    df[x_param],
    df[y_param],
    c=np.log10(df[color_param]),
    s=10,
    cmap="viridis",
    alpha=1,
)

# ax.legend(title="cu", loc='upper left', bbox_to_anchor=(1, 1))
colorbar = plt.colorbar(ax.collections[0], ax=ax)
colorbar.set_label(r"Log$_{10}$Volume Ratio ($\zeta_1$)")

ax.set_xlabel(r"Mass Ratio ($\phi_1$)")
ax.set_ylabel("Particles in Discrete Match ($N_{MDM}$)")
ax.set_xscale("log")
ax.set_yscale("log")

```

A similar pattern exists between $N_{MDM}$ , $\zeta_1$, and $I_C$ as shown in @fig-curvature_n.
At a given volume ratio, an increase in curvature index tends to result in an increased $N_{MDM}$.
For $I_C \approx 0.7$, $\log_{10}\zeta_1$ provides a very rough 1:1 correlation with $N_{MDM}$.
Curves with high $I_C$ tend to involve larger relative masses at lower sizes than those with low $I_C$.

```{python}
#| label: fig-curvature_N
#| fig-cap: "Total particles in MDM with curvature index. Color scale indicates volume ratio."

color_param = "vol_ratio"
x_param = "curvature_index"
y_param = "total_particles"

fig, ax = plt.subplots()
ax.scatter(
    df[x_param],
    df[y_param],
    c=np.log10(df[color_param]),
    s=10,
    cmap="viridis",
    alpha=1,
)

# ax.legend(title="cu", loc='upper left', bbox_to_anchor=(1, 1))
colorbar = plt.colorbar(ax.collections[0], ax=ax)
colorbar.set_label(r"Log$_{10}$Volume Ratio ($\zeta_1$)")

ax.set_xlabel("Curvature Index")
ax.set_ylabel("Particles in Discrete Match ($N_{MDM}$)")
# ax.set_xscale("log")
ax.set_yscale("log")

```

The grain size parameters $C_c$, $C_u$, and $I_{GS}$ were found to not correlate well with $N_{MDM}$.
However, $I_{GS}$ is helpful for visualizing the breadth of correlation between $N_{MDM}$ and USCS classifications.
@fig-uscs_n shows the range of $N_{MDM}$ associated with the granular USCS classifications in this study.
The lower boundary of the data results indicate that all but the broadest distributions have instances that can be matched with $N_{MDM}$ magnitudes of $\mathcal{O}(10^2)$ or lower.
Yet even the classifications with the smallest observed matches also have numerous instances requiring magnitudes from $\mathcal{O}(10^6)$ to $\mathcal{O}(10^9)$.

```{python}
#| label: fig-uscs_N
#| fig-cap: "Total particles in MDM with USCS classification."

fig, ax = plt.subplots(figsize=(5,7), layout="constrained")

x_param = "gs_index"
y_param = "total_particles"
group_param = "group_name"
# df_plot = df[df[group_param].str.contains("")]

categories = ["graded gravel", "graded sand", "silty"]
markers = ["1", ".", "."]

for j, cat in enumerate(categories):
    df_plot = df[df[group_param].str.contains(cat)]

    
    if cat == "silty":
        palette = ["#77B3C9", "#F2A900"]
    else:
        palette = sns.color_palette()  # "Set2", len(df_plot[group_param].unique()))

    for i, (label, group) in enumerate(df_plot.groupby(group_param)):
        # if "graded gravel" in label:
        #     marker = "|"
        # else:
        #     marker = "_"
        symbol = group["group_symbol"].iloc[0]
        ax.scatter(
            group[x_param],
            group[y_param],
            label=f"{symbol} - {label.capitalize()}",
            marker=markers[j],
            color=palette[i],
            # alpha=0.75,
            s=40,  # Scale point size by log_total_particles
        )


fig.legend(
    loc="outside upper center",
    fontsize="small",
    ncols=1,
    frameon=False,
    handlelength=2,
)

ax.set_xlabel("Grain Size Index")
ax.set_ylabel("Particles in Discrete Match ($N_{MDM}$)")
ax.set_ylim(1e0,2e10)
ax.set_xlim(0.0, 0.6)
# ax.set_xlim(1e-2, 1e2)
# ax.set_xscale("log")
ax.set_yscale("log")
ax.grid(True, which='both', axis='both')

```

Relative to "poorly graded gravel", "well-graded gravel" tends to have larger $N_{MDM}$.
Where "with sand" is applicable to the gravel group name, this tendancy and lower limit of $N_{MDM}$ remains the same, but the upper range of $N_{MDM}$ is higher.
Instances where "with silt and sand" are applicable to the gravel group name were rare in the generated GSDs and had very high values of $N_{MDM}$.
The rarity and large $N_{MDM}$ for these gradations are due to strict requirements for these group names, which only a small range of values across a broad range of particle sizes can satisfy.

Relative to "poorly graded sand", "well-graded sand" tends to have a similar (and quite broad) range of $N_{MDM}$.
The addition of "with gravel" or "with silt" makes very little difference in the range of $N_{MDM}$ at similar ranges of $I_{GS}$ (although a small increase in lower bound is apparent in "well-graded sand with gravel". Even "silty sand" appears to share orders of magnitude with "poorly graded sand" at comparable ranges of $I_{GS}$. Similar to the trends for gravels, sands whose group names include both silt and gravel were rare in the generated GSDs and had high values of $N_{MDM}$.

## Discussion

The $SI$ algorithm component of the approach presented here is significantly more effective than the $FS$ algorithm in finding the MDM.
However, whether this difference is critical in most geomechanics applications is unclear.
Intuitively, MDM solutions for distributions with smallest quantities of particles will have the most sensitivity to errors and seem to benefit from the $SI$ algorithm.
On the other hand, these distributions are likely to have the smallest volumes relative to their representative volumes and will therefore have $N_{sim} \gg N_{MDM}$, which will tend to reduce the errors associated with the $FS$ algorithm substantially.
Additionally, as it allows greater control over particle sizes, an approximate solution from the $FS$ algorithm may even be preferable depending on the application and the desired rigor in the $S:G$ match.

Both of the MDM approaches assume a single particle size between each adjacent sieve pair, but natural soil grain distributions are known to be broadly distributed between sieves [@ghalib1999].
This aspect of MDM solution stems from the use of $\text{Cond3}$, which is important in *finding* the solution, but can be abandoned after the solution is found in most cases.
Except where the solution contains only a single particle at a given size, the possibility exists for replacing the single size and associated quantity with an equal number of particles with varying (but allowable) sizes.
The greater the number of particles at a given size, the more potential there is to represent them with a distribution of sizes.

The comparison between predicted and reported $N_{sim}$ values in @fig-demo show the value of the MDM concept for experimental design.
As mentioned previously, differences between the reported and predicted values suggest the DEM models used a different discrete representation of the scaled GSDs than the MDM.
The fact that the reported values are consistently, though modestly, higher than the MDM predictions is expected since the MDM is defined to be a lower bound of possible values.
It is worth noting that the MDM concept is silent as to whether any valid discrete representation of a GSD is "better" than another, only which has fewest particles.
From a mechanics perspective, non-minimal GSD representations may have advantages that supersede increases in $N_{sim}$; an idea worth further study.

The MDM concept makes much of providing an accurate representation of grain size distribution because mechanical behavior cannot be fully decoupled from GSD.
However, when it is used in experimental design and to explore feasibility, it should be paired with the understanding that partial decoupling of mechanical behavior from GSD is sometimes possible.
For example, studies have shown that for certain combinations of GSD and density, there may be significant portions of the soil mass that are not mechanically engaged with the soil matrix [@sufian2021].
This may lead to thresholds of particle size that can be omitted from a given simulation without significant impact on the behavior being studied.
In such cases, the MDM concept is still valuable for comparing relative computational effort with varying levels of truncation.

## Conclusions

This paper described algorithms to 1) find a first-order approximation of a minimal discrete match (MDM) to general grain size distributions, 2) quantify the error in approximate minimal matches, and 3) find a rigorous solution for the MDM with a spanned integer approach.
The following conclusions are presented as the key out comes of the study on the MDM algorithms, comparison of predictions with independent DEM data, and a study on an array of GSDS:

1.  The MDM is a valuable tool in quantifying relative number of particles needed to create DEM models of different grain size distributions.
    This can provide important insight into computational cost and feasibility during experimental design.
    To be used effectively, it must be accompanied by an understanding of the representative volume concept.

2.  The MDM results for a large distribution of USCS show several trends: increasing the ratio of maximum to minimum particle size, the relative mass of the smallest particles, and the curvature index all increase the size of the MDM.

3.  The distribution of MDM size across USCS classifications indicates that a wide range of realistic granular soils are feasible for DEM modeling.
    The distribution also indicates that neither USCS classification, curvature index, size, and mass ratios alone are sufficient to characterize MDM size.
    This indicates that MDM magnitudes for specific GSDs of interest should calculated rather than estimated based on index parameters.

## Data and code availability

The Python code used to generate data, solve for minimal discrete matches, and plot figures are available on GitHub.
